{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "victor-doc_classification-BiLSTM_w2v.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNbLmwzvmzvZ29o3mWN/cqm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlimatds/victor-doc_classification/blob/main/victor_doc_classification_BiLSTM_w2v.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2FArtJT4g0B"
      },
      "source": [
        "## Document classification of Victor project using a BiSLTM as machine learning model\n",
        "\n",
        "We use pretrained word embeddings (word2vec, 300, skip-gram) from http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc\n",
        "\n",
        "The model has just one dense layer for its output.\n",
        "\n",
        "Deep learning library: PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G63vRCQ07kAQ"
      },
      "source": [
        "### Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch8SOJkB0m8d",
        "outputId": "ddbd72ae-cd35-41c3-973c-5b92f80e9071"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvjQa68e9Sv5"
      },
      "source": [
        "root_dir = '/content/gdrive/My Drive/'\n",
        "dataset_dir = root_dir + 'Machine Learning/Victor datasets/'\n",
        "word_embeddings_dir = dataset_dir + 'portuguese_word_vectors/'\n",
        "word_embeddings_file = word_embeddings_dir + 'w2v_skip_s300.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WAv1GXv7yTm"
      },
      "source": [
        "### Instaling dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW3NDy8i7y6m",
        "outputId": "a07b198d-cda2-4f9a-d1ba-c6ce49f40d49"
      },
      "source": [
        "!pip install tqdm\n",
        "!python -m spacy download pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Collecting pt_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: pt-core-news-sm\n",
            "  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.2.5-cp36-none-any.whl size=21186283 sha256=99597410e69085d29a1a9e77c819a0c4081a6a84c290682b88318f820c80b6df\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-25ie9wkk/wheels/ea/94/74/ec9be8418e9231b471be5dc7e1b45dd670019a376a6b5bc1c0\n",
            "Successfully built pt-core-news-sm\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/pt_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/pt\n",
            "You can now load the model via spacy.load('pt')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf8kZjOMDLCj",
        "outputId": "b60c053d-e066-4c87-c1b6-325b93b8c15b"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lU7PbT21LVP"
      },
      "source": [
        "import sys\n",
        "sys.path.append(word_embeddings_dir)\n",
        "import preprocessing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from gensim.models import KeyedVectors\n",
        "import zipfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDWUFn0BDl4l"
      },
      "source": [
        "### Application parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ5BbA9p2ULT"
      },
      "source": [
        "S = 500 # sentence length\n",
        "\n",
        "dataset_fraction = 0.6 # fraction of train and validation datasets to be used\n",
        "\n",
        "model_path = dataset_dir + 'LSTM+w2v/'\n",
        "model_file = model_path + f'pytorch_model-{S}-{dataset_fraction}.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeE2RUmLWFKj"
      },
      "source": [
        "### Load word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa5pnLyaWNGj",
        "outputId": "e614f393-cb1e-466b-d2f3-fa5ed24a3e39"
      },
      "source": [
        "%%time\n",
        "\n",
        "zip_file = zipfile.ZipFile(word_embeddings_file, 'r')\n",
        "path=zip_file.open('skip_s300.txt', 'r')\n",
        "gensim_model = KeyedVectors.load_word2vec_format(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 19s, sys: 1.67 s, total: 4min 21s\n",
            "Wall time: 4min 24s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfnTl_oYKSi4"
      },
      "source": [
        "### Loading and preprocessing datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpITBF9GOV_t"
      },
      "source": [
        "if dataset_fraction == 1.0: # full dataset\n",
        "  train_ds_file = 'train_small.csv'\n",
        "  validation_ds_file = 'validation_small.csv'\n",
        "else:\n",
        "  train_ds_file = f'train_small.csv-croped_{dataset_fraction}.csv'\n",
        "  validation_ds_file = f'validation_small.csv-croped_{dataset_fraction}.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GagKYGx_Nm74"
      },
      "source": [
        "import spacy\n",
        "\n",
        "spacy_pt = spacy.load('pt')\n",
        "\n",
        "def tokenizer(text):\n",
        "  clean_text = preprocessing.clean_text(text)\n",
        "  return [tok.text for tok in spacy_pt.tokenizer(clean_text)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xesa3Pnh3OMA",
        "outputId": "de29a574-0e7f-4685-f0da-78e67cbf1021"
      },
      "source": [
        "%%time\n",
        "\n",
        "TEXT = data.Field(\n",
        "    tokenize=tokenizer, \n",
        "    lower=True, \n",
        "    fix_length=S)\n",
        "LABEL = data.Field(\n",
        "    sequential=False, \n",
        "    unk_token=None)\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "    path=dataset_dir, \n",
        "    train=train_ds_file,\n",
        "    validation=validation_ds_file, \n",
        "    test='test_small.csv', \n",
        "    format='csv', \n",
        "    skip_header = True, \n",
        "    fields=[(None, None), (None, None), (None, None), ('label', LABEL), (None, None), ('text', TEXT)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 44s, sys: 2.74 s, total: 6min 46s\n",
            "Wall time: 6min 48s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIapB9iP3Wz4"
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks7cKltIALOK",
        "outputId": "91bb455e-be6b-473d-d6da-d950e2632c2a"
      },
      "source": [
        "W2V_SIZE = len(gensim_model.vectors[0])\n",
        "word2vec_vectors = []\n",
        "with tqdm(TEXT.vocab.stoi.items(), unit='token') as tepoch:\n",
        "  for token, idx in tepoch:\n",
        "    if token in gensim_model.vocab.keys():\n",
        "      word2vec_vectors.append(torch.FloatTensor(gensim_model[token]))\n",
        "    else:\n",
        "      word2vec_vectors.append(torch.zeros(W2V_SIZE))\n",
        "\n",
        "TEXT.vocab.set_vectors(TEXT.vocab.stoi, word2vec_vectors, W2V_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/148050 [00:00<?, ?token/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  \n",
            "100%|██████████| 148050/148050 [00:02<00:00, 63053.26token/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfELmTVb3chG"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "  (train_data, valid_data, test_data),\n",
        "  sort = False, #don't sort test/validation data\n",
        "  batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
        "  device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C-4iZDQD8YF"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DqTOBqf7DEW"
      },
      "source": [
        "class VictorBiLSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, sentence_len, embedding_weights):\n",
        "    super(VictorBiLSTM, self).__init__()\n",
        "    self.hidden_dim = 200\n",
        "    self.embedding_dim = embedding_weights.shape[1]\n",
        "    self.sentence_len = sentence_len\n",
        "    self.output_dim = 6\n",
        "\n",
        "    self.word_embeddings = nn.Embedding.from_pretrained(embedding_weights)\n",
        "    self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, bidirectional=True)\n",
        "    self.linear = nn.Linear(self.hidden_dim * sentence_len, self.output_dim)\n",
        "    self.dropout = nn.Dropout(0.20)\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    s_len = sentence.shape[0] #sentence length\n",
        "    b_len = sentence.shape[1] #batch size\n",
        "    embeds = self.word_embeddings(sentence) #embeds shape: (s_len, b_len, embedding_dim)\n",
        "    lstm_out, _ = self.lstm(embeds) #lstm_out shape: (s_len, b_len, 2 * hidden_dim) => the number 2 comes because the layer is bidirectional\n",
        "    sum = (\n",
        "        lstm_out[:, :, :self.hidden_dim] +  # hidden states from forward layer \n",
        "        lstm_out[:, :, self.hidden_dim:])   # hidden states from backward layer \n",
        "    linear_input = torch.flatten(sum.transpose(0, 1), start_dim=1) #linear_input shape: (b_len, s_len * hidden_dim)\n",
        "    linear_input = self.dropout(linear_input)\n",
        "    x = self.linear(linear_input)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx1m1uo4PWbK"
      },
      "source": [
        "### Training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21I1c-2tPZaS"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def compute_metrics(targets, predictions):\n",
        "  f1_macro = f1_score(targets, np.argmax(predictions, axis=1), average='macro')\n",
        "  f1_weighted = f1_score(targets, np.argmax(predictions, axis=1), average='weighted')\n",
        "  return f1_macro, f1_weighted\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, epoch):\n",
        "  epoch_loss = 0\n",
        "  model.train()\n",
        "  with tqdm(iterator, unit='batch') as tepoch:\n",
        "    for batch in tepoch:\n",
        "      tepoch.set_description(f\"Epoch {epoch+1} (train)\")\n",
        "      optimizer.zero_grad()\n",
        "      predictions = model(batch.text).squeeze(1)\n",
        "      loss = criterion(predictions, batch.label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "      tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def predict(model, iterator):\n",
        "  model.eval()\n",
        "  predictions = None\n",
        "  targets = None\n",
        "  with torch.no_grad():\n",
        "    with tqdm(iterator, unit=' batch') as tbatch:\n",
        "      for batch in tbatch:\n",
        "        tbatch.set_description(f'Predicting ')\n",
        "        out = model(batch.text).squeeze(1)\n",
        "        if predictions == None:\n",
        "          predictions = out\n",
        "          targets = batch.label\n",
        "        else:\n",
        "          predictions = torch.cat([predictions, out], dim=0)\n",
        "          targets = torch.cat([targets, batch.label], dim=0)\n",
        "  \n",
        "  return predictions.cpu().numpy(), targets.cpu().numpy()\n",
        "\n",
        "def evaluate(model, iterator):  \n",
        "  predictions, targets = predict(model, iterator)\n",
        "  return compute_metrics(targets, predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEi8r8-VPgOK"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqyhx-vh7_rJ"
      },
      "source": [
        "EPOCHS = 20\n",
        "learning_rate = 1e-3\n",
        "\n",
        "model = VictorBiLSTM(S, TEXT.vocab.vectors)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ojydCaIgFzf-",
        "outputId": "7f2c1c6f-7900-4e31-906a-01f7f8998682"
      },
      "source": [
        "%%time\n",
        "import pandas as pd\n",
        "from IPython.display import display, update_display\n",
        "\n",
        "metrics_df = pd.DataFrame(columns=['Epoch', 'Train Loss', 'Validation F1 macro', 'Validation F1 weighted'])\n",
        "metrics_display = display(metrics_df, display_id='metrics_table')\n",
        "\n",
        "best_valid_f1 = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss = train(model, train_iterator, optimizer, criterion, epoch)\n",
        "  valid_f1_m, valid_f1_w = evaluate(model, valid_iterator)\n",
        "  \n",
        "  #saving\n",
        "  if valid_f1_m > best_valid_f1:\n",
        "    best_valid_f1 = valid_f1_m\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "\n",
        "  #printing\n",
        "  metrics_df.loc[epoch] = [epoch + 1, train_loss, valid_f1_m, valid_f1_w]\n",
        "  metrics_display.update(metrics_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Train Loss</th>\n",
              "      <th>Validation F1 macro</th>\n",
              "      <th>Validation F1 weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.330300</td>\n",
              "      <td>0.680275</td>\n",
              "      <td>0.894597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.183845</td>\n",
              "      <td>0.730629</td>\n",
              "      <td>0.906515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.122720</td>\n",
              "      <td>0.756013</td>\n",
              "      <td>0.913665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0.099780</td>\n",
              "      <td>0.751363</td>\n",
              "      <td>0.913194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.089001</td>\n",
              "      <td>0.759100</td>\n",
              "      <td>0.911326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.080008</td>\n",
              "      <td>0.761584</td>\n",
              "      <td>0.917467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.073276</td>\n",
              "      <td>0.765631</td>\n",
              "      <td>0.915173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8.0</td>\n",
              "      <td>0.068845</td>\n",
              "      <td>0.761339</td>\n",
              "      <td>0.916512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9.0</td>\n",
              "      <td>0.065283</td>\n",
              "      <td>0.757974</td>\n",
              "      <td>0.917758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.060705</td>\n",
              "      <td>0.771743</td>\n",
              "      <td>0.919248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11.0</td>\n",
              "      <td>0.057351</td>\n",
              "      <td>0.768489</td>\n",
              "      <td>0.918879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12.0</td>\n",
              "      <td>0.055801</td>\n",
              "      <td>0.768345</td>\n",
              "      <td>0.919525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13.0</td>\n",
              "      <td>0.052949</td>\n",
              "      <td>0.773030</td>\n",
              "      <td>0.918739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14.0</td>\n",
              "      <td>0.050807</td>\n",
              "      <td>0.766681</td>\n",
              "      <td>0.919754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15.0</td>\n",
              "      <td>0.048497</td>\n",
              "      <td>0.767245</td>\n",
              "      <td>0.919671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16.0</td>\n",
              "      <td>0.048216</td>\n",
              "      <td>0.765574</td>\n",
              "      <td>0.918647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.045769</td>\n",
              "      <td>0.771379</td>\n",
              "      <td>0.919446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.043743</td>\n",
              "      <td>0.769946</td>\n",
              "      <td>0.919803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19.0</td>\n",
              "      <td>0.042604</td>\n",
              "      <td>0.773845</td>\n",
              "      <td>0.919724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20.0</td>\n",
              "      <td>0.042130</td>\n",
              "      <td>0.762697</td>\n",
              "      <td>0.920145</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Epoch  Train Loss  Validation F1 macro  Validation F1 weighted\n",
              "0     1.0    0.330300             0.680275                0.894597\n",
              "1     2.0    0.183845             0.730629                0.906515\n",
              "2     3.0    0.122720             0.756013                0.913665\n",
              "3     4.0    0.099780             0.751363                0.913194\n",
              "4     5.0    0.089001             0.759100                0.911326\n",
              "5     6.0    0.080008             0.761584                0.917467\n",
              "6     7.0    0.073276             0.765631                0.915173\n",
              "7     8.0    0.068845             0.761339                0.916512\n",
              "8     9.0    0.065283             0.757974                0.917758\n",
              "9    10.0    0.060705             0.771743                0.919248\n",
              "10   11.0    0.057351             0.768489                0.918879\n",
              "11   12.0    0.055801             0.768345                0.919525\n",
              "12   13.0    0.052949             0.773030                0.918739\n",
              "13   14.0    0.050807             0.766681                0.919754\n",
              "14   15.0    0.048497             0.767245                0.919671\n",
              "15   16.0    0.048216             0.765574                0.918647\n",
              "16   17.0    0.045769             0.771379                0.919446\n",
              "17   18.0    0.043743             0.769946                0.919803\n",
              "18   19.0    0.042604             0.773845                0.919724\n",
              "19   20.0    0.042130             0.762697                0.920145"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 (train): 100%|██████████| 2987/2987 [02:30<00:00, 19.90batch/s, loss=0.187]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 46.63 batch/s]\n",
            "Epoch 2 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.30batch/s, loss=0.0739]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 46.76 batch/s]\n",
            "Epoch 3 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.32batch/s, loss=0.355]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 47.41 batch/s]\n",
            "Epoch 4 (train): 100%|██████████| 2987/2987 [02:35<00:00, 19.20batch/s, loss=0.0248]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 47.28 batch/s]\n",
            "Epoch 5 (train): 100%|██████████| 2987/2987 [02:35<00:00, 19.26batch/s, loss=0.0411]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 47.11 batch/s]\n",
            "Epoch 6 (train): 100%|██████████| 2987/2987 [02:35<00:00, 19.20batch/s, loss=0.00794]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 47.06 batch/s]\n",
            "Epoch 7 (train): 100%|██████████| 2987/2987 [02:35<00:00, 19.25batch/s, loss=0.219]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:41<00:00, 46.03 batch/s]\n",
            "Epoch 8 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.29batch/s, loss=0.278]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:41<00:00, 46.55 batch/s]\n",
            "Epoch 9 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.36batch/s, loss=0.00226]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 47.48 batch/s]\n",
            "Epoch 10 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.40batch/s, loss=0.00388]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 47.24 batch/s]\n",
            "Epoch 11 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.39batch/s, loss=0.0114]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 47.05 batch/s]\n",
            "Epoch 12 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.38batch/s, loss=0.018]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:39<00:00, 47.81 batch/s]\n",
            "Epoch 13 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.30batch/s, loss=0.195]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 47.32 batch/s]\n",
            "Epoch 14 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.35batch/s, loss=0.135]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:39<00:00, 48.35 batch/s]\n",
            "Epoch 15 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.31batch/s, loss=0.00341]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:39<00:00, 48.59 batch/s]\n",
            "Epoch 16 (train): 100%|██████████| 2987/2987 [02:35<00:00, 19.27batch/s, loss=0.188]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:39<00:00, 47.85 batch/s]\n",
            "Epoch 17 (train): 100%|██████████| 2987/2987 [02:33<00:00, 19.42batch/s, loss=0.112]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 46.81 batch/s]\n",
            "Epoch 18 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.27batch/s, loss=0.00625]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:40<00:00, 46.98 batch/s]\n",
            "Epoch 19 (train): 100%|██████████| 2987/2987 [02:36<00:00, 19.12batch/s, loss=0.0511]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:41<00:00, 45.74 batch/s]\n",
            "Epoch 20 (train): 100%|██████████| 2987/2987 [02:34<00:00, 19.29batch/s, loss=0.135]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:39<00:00, 48.04 batch/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 59min 21s, sys: 4min 20s, total: 1h 3min 41s\n",
            "Wall time: 1h 5min 10s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdLzn3y0Pz-C"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzcH6UOuP1sJ"
      },
      "source": [
        "def load_saved_model(file_name):\n",
        "  m = VictorBiLSTM(S, TEXT.vocab.vectors)\n",
        "  m = m.to(device)\n",
        "  m.load_state_dict(torch.load(file_name, map_location=device))\n",
        "  m.eval()\n",
        "  return m\n",
        "\n",
        "# If the next line is keep commmented, it will load the saved model\n",
        "#model_file = model_path + 'pytorch_model-500-1.0.pt'\n",
        "model = load_saved_model(model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_E8o57eP8fC",
        "outputId": "31b18c31-47fc-42f0-e07b-d9c057bf56f1"
      },
      "source": [
        "predictions, targets = predict(model, test_iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting : 100%|██████████| 2986/2986 [01:03<00:00, 47.38 batch/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xj9twKIP-5D",
        "outputId": "4c6f4312-c3e9-4f5b-d3e8-e725d339d460"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(\n",
        "    targets, \n",
        "    np.argmax(predictions, axis=1), \n",
        "    digits=4, \n",
        "    target_names=LABEL.vocab.itos)\n",
        "\n",
        "print(report)\n",
        "\n",
        "rep_file = open(model_file + \"-test_report.txt\", \"wt\")\n",
        "rep_file.write(report + '\\n')\n",
        "rep_file.write(f'learning rate: {learning_rate}\\n')\n",
        "rep_file.write(f'optimizer: {type(optimizer).__name__}\\n')\n",
        "rep_file.write(f'criterion: {type(criterion).__name__}\\n')\n",
        "rep_file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                  precision    recall  f1-score   support\n",
            "\n",
            "                          outros     0.9658    0.9691    0.9675     85408\n",
            "                   peticao_do_RE     0.7083    0.7272    0.7176      6331\n",
            "agravo_em_recurso_extraordinario     0.6074    0.5269    0.5643      1841\n",
            "                        sentenca     0.7874    0.6956    0.7387      1475\n",
            "          acordao_de_2_instancia     0.8710    0.8901    0.8804       273\n",
            "     despacho_de_admissibilidade     0.6929    0.4899    0.5740       198\n",
            "\n",
            "                        accuracy                         0.9391     95526\n",
            "                       macro avg     0.7721    0.7165    0.7404     95526\n",
            "                    weighted avg     0.9382    0.9391    0.9385     95526\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njWM1Lr1_5yn"
      },
      "source": [
        "References:\n",
        "\n",
        "- https://medium.com/@rohit_agrawal/using-fine-tuned-gensim-word2vec-embeddings-with-torchtext-and-pytorch-17eea2883cd\n"
      ]
    }
  ]
}