{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "victor-doc_classification-BiLSTM_w2v-2_dense_layers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMEfFiJsKgm9+ANWEmpA6eT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlimatds/victor-doc_classification/blob/main/victor_doc_classification_BiLSTM_w2v_2_dense_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2FArtJT4g0B"
      },
      "source": [
        "## Document classification of Victor project using a BiSLTM as machine learning model\n",
        "\n",
        "We use pretrained word embeddings (word2vec, 300, skip-gram) from http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc\n",
        "\n",
        "The model has two dense layers for its output.\n",
        "\n",
        "Deep learning library: PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G63vRCQ07kAQ"
      },
      "source": [
        "### Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch8SOJkB0m8d",
        "outputId": "03517d81-7d28-4d17-b309-f32254394179"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvjQa68e9Sv5"
      },
      "source": [
        "root_dir = '/content/gdrive/My Drive/'\n",
        "dataset_dir = root_dir + 'Machine Learning/Victor datasets/'\n",
        "word_embeddings_dir = dataset_dir + 'portuguese_word_vectors/'\n",
        "word_embeddings_file = word_embeddings_dir + 'w2v_skip_s300.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WAv1GXv7yTm"
      },
      "source": [
        "### Instaling dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XW3NDy8i7y6m",
        "outputId": "96aaa71c-1f30-4c1f-a468-75b606c1dbc7"
      },
      "source": [
        "!pip install tqdm\n",
        "!python -m spacy download pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: pt_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz#egg=pt_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (51.3.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/pt_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/pt\n",
            "You can now load the model via spacy.load('pt')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf8kZjOMDLCj",
        "outputId": "166d8498-b28e-4c42-b958-27f4deca6f7c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lU7PbT21LVP"
      },
      "source": [
        "import sys\n",
        "sys.path.append(word_embeddings_dir)\n",
        "import preprocessing\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext import data\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from gensim.models import KeyedVectors\n",
        "import zipfile\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDWUFn0BDl4l"
      },
      "source": [
        "### Application parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ5BbA9p2ULT"
      },
      "source": [
        "S = 500 # sentence length\n",
        "\n",
        "dataset_fraction = 0.6 # fraction of train and validation datasets to be used\n",
        "\n",
        "model_path = dataset_dir + 'LSTM+w2v+2dense_layers/'\n",
        "model_file = model_path + f'pytorch_model-{S}-{dataset_fraction}.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeE2RUmLWFKj"
      },
      "source": [
        "### Load word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa5pnLyaWNGj",
        "outputId": "5a9e4966-1de0-4ec9-bc20-cccb25715b20"
      },
      "source": [
        "%%time\n",
        "\n",
        "zip_file = zipfile.ZipFile(word_embeddings_file, 'r')\n",
        "path=zip_file.open('skip_s300.txt', 'r')\n",
        "gensim_model = KeyedVectors.load_word2vec_format(path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4min 5s, sys: 1.42 s, total: 4min 7s\n",
            "Wall time: 4min 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfnTl_oYKSi4"
      },
      "source": [
        "### Loading and preprocessing datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpITBF9GOV_t"
      },
      "source": [
        "if dataset_fraction == 1.0: # full dataset\n",
        "  train_ds_file = 'train_small.csv'\n",
        "  validation_ds_file = 'validation_small.csv'\n",
        "else:\n",
        "  train_ds_file = f'train_small.csv-croped_{dataset_fraction}.csv'\n",
        "  validation_ds_file = f'validation_small.csv-croped_{dataset_fraction}.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GagKYGx_Nm74"
      },
      "source": [
        "import spacy\n",
        "\n",
        "spacy_pt = spacy.load('pt')\n",
        "\n",
        "def tokenizer(text):\n",
        "  clean_text = preprocessing.clean_text(text)\n",
        "  return [tok.text for tok in spacy_pt.tokenizer(clean_text)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xesa3Pnh3OMA",
        "outputId": "d93cce57-1718-465d-d81a-780f27edf162"
      },
      "source": [
        "%%time\n",
        "\n",
        "TEXT = data.Field(\n",
        "    tokenize=tokenizer, \n",
        "    lower=True, \n",
        "    fix_length=S)\n",
        "LABEL = data.Field(\n",
        "    sequential=False, \n",
        "    unk_token=None)\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "    path=dataset_dir, \n",
        "    train=train_ds_file,\n",
        "    validation=validation_ds_file, \n",
        "    test='test_small.csv', \n",
        "    format='csv', \n",
        "    skip_header = True, \n",
        "    fields=[(None, None), (None, None), (None, None), ('label', LABEL), (None, None), ('text', TEXT)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6min 40s, sys: 2.43 s, total: 6min 42s\n",
            "Wall time: 6min 42s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIapB9iP3Wz4"
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks7cKltIALOK",
        "outputId": "c78821d0-01ed-404d-85db-bb0d3c8b5fab"
      },
      "source": [
        "W2V_SIZE = len(gensim_model.vectors[0])\n",
        "word2vec_vectors = []\n",
        "with tqdm(TEXT.vocab.stoi.items(), unit='token') as tepoch:\n",
        "  for token, idx in tepoch:\n",
        "    if token in gensim_model.vocab.keys():\n",
        "      word2vec_vectors.append(torch.FloatTensor(gensim_model[token]))\n",
        "    else:\n",
        "      word2vec_vectors.append(torch.zeros(W2V_SIZE))\n",
        "\n",
        "TEXT.vocab.set_vectors(TEXT.vocab.stoi, word2vec_vectors, W2V_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/148050 [00:00<?, ?token/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  \n",
            "100%|██████████| 148050/148050 [00:02<00:00, 64547.17token/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfELmTVb3chG"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "  (train_data, valid_data, test_data),\n",
        "  sort = False, #don't sort test/validation data\n",
        "  batch_sizes=(BATCH_SIZE, BATCH_SIZE, BATCH_SIZE),\n",
        "  device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C-4iZDQD8YF"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DqTOBqf7DEW"
      },
      "source": [
        "class VictorBiLSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, sentence_len, embedding_weights):\n",
        "    super(VictorBiLSTM, self).__init__()\n",
        "    self.hidden_dim = 200\n",
        "    self.embedding_dim = embedding_weights.shape[1]\n",
        "    self.sentence_len = sentence_len\n",
        "    self.output_dim = 6\n",
        "\n",
        "    self.word_embeddings = nn.Embedding.from_pretrained(embedding_weights)\n",
        "    self.lstm = nn.LSTM(self.embedding_dim, self.hidden_dim, bidirectional=True)\n",
        "    linear_1_out_dim = 60\n",
        "    self.linear_1 = nn.Linear(self.hidden_dim * sentence_len, linear_1_out_dim)\n",
        "    self.linear_2 = nn.Linear(linear_1_out_dim, self.output_dim)\n",
        "    self.dropout = nn.Dropout(0.20)\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    s_len = sentence.shape[0] #sentence length\n",
        "    b_len = sentence.shape[1] #batch size\n",
        "    embeds = self.word_embeddings(sentence) #embeds shape: (s_len, b_len, embedding_dim)\n",
        "    lstm_out, _ = self.lstm(embeds) #lstm_out shape: (s_len, b_len, 2 * hidden_dim) => the number 2 comes because the layer is bidirectional\n",
        "    sum = (\n",
        "        lstm_out[:, :, :self.hidden_dim] +  # hidden states from forward layer \n",
        "        lstm_out[:, :, self.hidden_dim:])   # hidden states from backward layer \n",
        "    linear_input = torch.flatten(sum.transpose(0, 1), start_dim=1) #linear_input shape: (b_len, s_len * hidden_dim)\n",
        "    x = self.linear_1(linear_input)\n",
        "    x = self.dropout(x)\n",
        "    x = self.linear_2(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx1m1uo4PWbK"
      },
      "source": [
        "### Training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21I1c-2tPZaS"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def compute_metrics(targets, predictions):\n",
        "  f1_macro = f1_score(targets, np.argmax(predictions, axis=1), average='macro')\n",
        "  f1_weighted = f1_score(targets, np.argmax(predictions, axis=1), average='weighted')\n",
        "  return f1_macro, f1_weighted\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, epoch):\n",
        "  epoch_loss = 0\n",
        "  model.train()\n",
        "  with tqdm(iterator, unit='batch') as tepoch:\n",
        "    for batch in tepoch:\n",
        "      tepoch.set_description(f\"Epoch {epoch+1} (train)\")\n",
        "      optimizer.zero_grad()\n",
        "      predictions = model(batch.text).squeeze(1)\n",
        "      loss = criterion(predictions, batch.label)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "      tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "  return epoch_loss / len(iterator)\n",
        "\n",
        "def predict(model, iterator):\n",
        "  model.eval()\n",
        "  predictions = None\n",
        "  targets = None\n",
        "  with torch.no_grad():\n",
        "    with tqdm(iterator, unit=' batch') as tbatch:\n",
        "      for batch in tbatch:\n",
        "        tbatch.set_description(f'Predicting ')\n",
        "        out = model(batch.text).squeeze(1)\n",
        "        if predictions == None:\n",
        "          predictions = out\n",
        "          targets = batch.label\n",
        "        else:\n",
        "          predictions = torch.cat([predictions, out], dim=0)\n",
        "          targets = torch.cat([targets, batch.label], dim=0)\n",
        "  \n",
        "  return predictions.cpu().numpy(), targets.cpu().numpy()\n",
        "\n",
        "def evaluate(model, iterator):  \n",
        "  predictions, targets = predict(model, iterator)\n",
        "  return compute_metrics(targets, predictions)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEi8r8-VPgOK"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqyhx-vh7_rJ"
      },
      "source": [
        "EPOCHS = 20\n",
        "learning_rate = 1e-3\n",
        "\n",
        "model = VictorBiLSTM(S, TEXT.vocab.vectors)\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ojydCaIgFzf-",
        "outputId": "5e9d3d95-ee5b-4d28-9e97-7e64ed6f5210"
      },
      "source": [
        "%%time\n",
        "import pandas as pd\n",
        "from IPython.display import display, update_display\n",
        "\n",
        "metrics_df = pd.DataFrame(columns=['Epoch', 'Train Loss', 'Validation F1 macro', 'Validation F1 weighted'])\n",
        "metrics_display = display(metrics_df, display_id='metrics_table')\n",
        "\n",
        "best_valid_f1 = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_loss = train(model, train_iterator, optimizer, criterion, epoch)\n",
        "  valid_f1_m, valid_f1_w = evaluate(model, valid_iterator)\n",
        "  \n",
        "  #saving\n",
        "  if valid_f1_m > best_valid_f1:\n",
        "    best_valid_f1 = valid_f1_m\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "\n",
        "  #printing\n",
        "  metrics_df.loc[epoch] = [epoch + 1, train_loss, valid_f1_m, valid_f1_w]\n",
        "  metrics_display.update(metrics_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Train Loss</th>\n",
              "      <th>Validation F1 macro</th>\n",
              "      <th>Validation F1 weighted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.336599</td>\n",
              "      <td>0.647420</td>\n",
              "      <td>0.886111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.186816</td>\n",
              "      <td>0.711036</td>\n",
              "      <td>0.907192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.0</td>\n",
              "      <td>0.124331</td>\n",
              "      <td>0.751280</td>\n",
              "      <td>0.914744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.0</td>\n",
              "      <td>0.100247</td>\n",
              "      <td>0.759498</td>\n",
              "      <td>0.908327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.088821</td>\n",
              "      <td>0.739017</td>\n",
              "      <td>0.916401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.081283</td>\n",
              "      <td>0.754969</td>\n",
              "      <td>0.917255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7.0</td>\n",
              "      <td>0.072761</td>\n",
              "      <td>0.764966</td>\n",
              "      <td>0.916399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8.0</td>\n",
              "      <td>0.067638</td>\n",
              "      <td>0.759084</td>\n",
              "      <td>0.918686</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9.0</td>\n",
              "      <td>0.064443</td>\n",
              "      <td>0.760752</td>\n",
              "      <td>0.920088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.059319</td>\n",
              "      <td>0.772527</td>\n",
              "      <td>0.919066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11.0</td>\n",
              "      <td>0.059376</td>\n",
              "      <td>0.767956</td>\n",
              "      <td>0.919623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12.0</td>\n",
              "      <td>0.054960</td>\n",
              "      <td>0.772375</td>\n",
              "      <td>0.921864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13.0</td>\n",
              "      <td>0.051621</td>\n",
              "      <td>0.765195</td>\n",
              "      <td>0.918950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14.0</td>\n",
              "      <td>0.048546</td>\n",
              "      <td>0.771107</td>\n",
              "      <td>0.919487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15.0</td>\n",
              "      <td>0.048136</td>\n",
              "      <td>0.765335</td>\n",
              "      <td>0.920732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16.0</td>\n",
              "      <td>0.046379</td>\n",
              "      <td>0.768243</td>\n",
              "      <td>0.920353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.044000</td>\n",
              "      <td>0.768864</td>\n",
              "      <td>0.921249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18.0</td>\n",
              "      <td>0.040478</td>\n",
              "      <td>0.766547</td>\n",
              "      <td>0.921004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19.0</td>\n",
              "      <td>0.042391</td>\n",
              "      <td>0.770541</td>\n",
              "      <td>0.920244</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Epoch  Train Loss  Validation F1 macro  Validation F1 weighted\n",
              "0     1.0    0.336599             0.647420                0.886111\n",
              "1     2.0    0.186816             0.711036                0.907192\n",
              "2     3.0    0.124331             0.751280                0.914744\n",
              "3     4.0    0.100247             0.759498                0.908327\n",
              "4     5.0    0.088821             0.739017                0.916401\n",
              "5     6.0    0.081283             0.754969                0.917255\n",
              "6     7.0    0.072761             0.764966                0.916399\n",
              "7     8.0    0.067638             0.759084                0.918686\n",
              "8     9.0    0.064443             0.760752                0.920088\n",
              "9    10.0    0.059319             0.772527                0.919066\n",
              "10   11.0    0.059376             0.767956                0.919623\n",
              "11   12.0    0.054960             0.772375                0.921864\n",
              "12   13.0    0.051621             0.765195                0.918950\n",
              "13   14.0    0.048546             0.771107                0.919487\n",
              "14   15.0    0.048136             0.765335                0.920732\n",
              "15   16.0    0.046379             0.768243                0.920353\n",
              "16   17.0    0.044000             0.768864                0.921249\n",
              "17   18.0    0.040478             0.766547                0.921004\n",
              "18   19.0    0.042391             0.770541                0.920244"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 (train): 100%|██████████| 2987/2987 [02:58<00:00, 16.75batch/s, loss=0.0378]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 44.69 batch/s]\n",
            "Epoch 2 (train): 100%|██████████| 2987/2987 [02:58<00:00, 16.74batch/s, loss=0.0555]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 44.79 batch/s]\n",
            "Epoch 3 (train): 100%|██████████| 2987/2987 [02:57<00:00, 16.79batch/s, loss=0.148]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:43<00:00, 44.30 batch/s]\n",
            "Epoch 4 (train): 100%|██████████| 2987/2987 [02:57<00:00, 16.86batch/s, loss=0.0448]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 44.80 batch/s]\n",
            "Epoch 5 (train): 100%|██████████| 2987/2987 [02:56<00:00, 16.94batch/s, loss=0.0107]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 44.62 batch/s]\n",
            "Epoch 6 (train): 100%|██████████| 2987/2987 [02:58<00:00, 16.70batch/s, loss=0.157]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:43<00:00, 44.13 batch/s]\n",
            "Epoch 7 (train): 100%|██████████| 2987/2987 [02:58<00:00, 16.71batch/s, loss=0.0226]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 44.46 batch/s]\n",
            "Epoch 8 (train): 100%|██████████| 2987/2987 [02:57<00:00, 16.82batch/s, loss=0.0167]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 44.68 batch/s]\n",
            "Epoch 9 (train): 100%|██████████| 2987/2987 [02:56<00:00, 16.90batch/s, loss=0.00787]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:43<00:00, 44.24 batch/s]\n",
            "Epoch 10 (train): 100%|██████████| 2987/2987 [02:57<00:00, 16.83batch/s, loss=0.00781]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 45.43 batch/s]\n",
            "Epoch 11 (train): 100%|██████████| 2987/2987 [02:54<00:00, 17.08batch/s, loss=0.00489]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 44.95 batch/s]\n",
            "Epoch 12 (train): 100%|██████████| 2987/2987 [02:56<00:00, 16.89batch/s, loss=0.0147]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 44.85 batch/s]\n",
            "Epoch 13 (train): 100%|██████████| 2987/2987 [02:56<00:00, 16.97batch/s, loss=0.00513]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:41<00:00, 45.49 batch/s]\n",
            "Epoch 14 (train): 100%|██████████| 2987/2987 [02:55<00:00, 17.03batch/s, loss=0.0126]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 44.76 batch/s]\n",
            "Epoch 15 (train): 100%|██████████| 2987/2987 [02:55<00:00, 16.99batch/s, loss=0.00674]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 45.05 batch/s]\n",
            "Epoch 16 (train): 100%|██████████| 2987/2987 [02:55<00:00, 16.98batch/s, loss=0.0102]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 44.72 batch/s]\n",
            "Epoch 17 (train): 100%|██████████| 2987/2987 [02:56<00:00, 16.90batch/s, loss=0.00929]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 44.62 batch/s]\n",
            "Epoch 18 (train): 100%|██████████| 2987/2987 [02:55<00:00, 17.03batch/s, loss=0.00653]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 45.32 batch/s]\n",
            "Epoch 19 (train): 100%|██████████| 2987/2987 [02:55<00:00, 17.07batch/s, loss=0.0296]\n",
            "Predicting : 100%|██████████| 1910/1910 [00:42<00:00, 45.33 batch/s]\n",
            "Epoch 20 (train):  75%|███████▌  | 2255/2987 [02:12<00:43, 16.68batch/s, loss=0.0181]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdLzn3y0Pz-C"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzcH6UOuP1sJ"
      },
      "source": [
        "def load_saved_model(file_name):\n",
        "  m = VictorBiLSTM(S, TEXT.vocab.vectors)\n",
        "  m = m.to(device)\n",
        "  m.load_state_dict(torch.load(file_name, map_location=device))\n",
        "  m.eval()\n",
        "  return m\n",
        "\n",
        "# If the next line is keep commmented, it will load the saved model\n",
        "#model_file = model_path + 'pytorch_model-500-1.0.pt'\n",
        "model = load_saved_model(model_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_E8o57eP8fC",
        "outputId": "2bbb73e2-d759-4037-9b53-4701f39d1927"
      },
      "source": [
        "predictions, targets = predict(model, test_iterator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting : 100%|██████████| 2986/2986 [01:06<00:00, 44.94 batch/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Xj9twKIP-5D",
        "outputId": "7e4e428a-26e7-4bfe-d1a3-245dee9b9fb6"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(\n",
        "    targets, \n",
        "    np.argmax(predictions, axis=1), \n",
        "    digits=4, \n",
        "    target_names=LABEL.vocab.itos)\n",
        "\n",
        "print(report)\n",
        "\n",
        "rep_file = open(model_file + \"-test_report.txt\", \"wt\")\n",
        "rep_file.write(report + '\\n')\n",
        "rep_file.write(f'learning rate: {learning_rate}\\n')\n",
        "rep_file.write(f'optimizer: {type(optimizer).__name__}\\n')\n",
        "rep_file.write(f'criterion: {type(criterion).__name__}\\n')\n",
        "rep_file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                  precision    recall  f1-score   support\n",
            "\n",
            "                          outros     0.9663    0.9648    0.9656     85408\n",
            "                   peticao_do_RE     0.6806    0.7350    0.7067      6331\n",
            "agravo_em_recurso_extraordinario     0.5691    0.5660    0.5675      1841\n",
            "                        sentenca     0.8370    0.6685    0.7433      1475\n",
            "          acordao_de_2_instancia     0.9067    0.8901    0.8983       273\n",
            "     despacho_de_admissibilidade     0.6618    0.4545    0.5389       198\n",
            "\n",
            "                        accuracy                         0.9361     95526\n",
            "                       macro avg     0.7702    0.7132    0.7367     95526\n",
            "                    weighted avg     0.9369    0.9361    0.9362     95526\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njWM1Lr1_5yn"
      },
      "source": [
        "References:\n",
        "\n",
        "- https://medium.com/@rohit_agrawal/using-fine-tuned-gensim-word2vec-embeddings-with-torchtext-and-pytorch-17eea2883cd\n"
      ]
    }
  ]
}