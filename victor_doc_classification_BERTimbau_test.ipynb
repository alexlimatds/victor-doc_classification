{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "victor-doc_classification-BERTimbau-test.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlimatds/victor-doc_classification/blob/main/victor_doc_classification_BERTimbau_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZcSW6EN-VWN"
      },
      "source": [
        "## Document classification of Victor project using BERTimbau as machine learning model\n",
        "\n",
        "Because of memory limitations, this notebook runs just the test. For the train, check the specific notebook.\n",
        "\n",
        "Application based on the Hugging Face library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwww4AUYqZz0"
      },
      "source": [
        "### Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOUL3OMpBhzh",
        "outputId": "5b83d281-f44c-44c7-ba29-40a81c7a55a8"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.2.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from datasets) (3.4.0)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMQIxnsrayju"
      },
      "source": [
        "### Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U53WC8tpresl",
        "outputId": "78bacae0-689f-437b-b9ca-dafb7751ce89"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = '/content/gdrive/My Drive/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lHrMdQNrr2Q"
      },
      "source": [
        "dataset_dir = root_dir + 'Machine Learning/Victor datasets/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN-sQ26kjxXH"
      },
      "source": [
        "### Application parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2qLiXL0juUQ"
      },
      "source": [
        "model_path = dataset_dir + 'BERTimbau250-0.6/'\n",
        "\n",
        "S = 250 # Maximum number of tokens in a sentence\n",
        "\n",
        "# dataset file\n",
        "#test_ds_file = 'train_small.csv-croped_0.6.csv'\n",
        "test_ds_file = 'test_small.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfzEBprbzvfz"
      },
      "source": [
        "### Loading saved model and its tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKRd0C5nzvCN"
      },
      "source": [
        "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
        "\n",
        "# Using the fine-tuned model\n",
        "model_name = 'neuralmind/bert-base-portuguese-cased'\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85JYe9N5z9mQ"
      },
      "source": [
        "id2label = {\n",
        "    0: 'acordao_de_2_instancia', \n",
        "    1: 'agravo_em_recurso_extraordinario', \n",
        "    2: 'despacho_de_admissibilidade', \n",
        "    3: 'outros', \n",
        "    4: 'peticao_do_RE', \n",
        "    5: 'sentenca'}\n",
        "label2id = {v : k for (k, v) in id2label.items()}\n",
        "\n",
        "model.config.id2label = id2label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OLwQEraqd-R"
      },
      "source": [
        "### Loading and preprocessing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK1_fn8IpXCz"
      },
      "source": [
        "import torch\n",
        "import torchtext\n",
        "\n",
        "class VictorDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings = encodings\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    ids = torch.tensor(self.encodings[idx]['input_ids'], dtype=torch.long)\n",
        "    mask = torch.tensor(self.encodings[idx]['attention_mask'], dtype=torch.long)\n",
        "    return {\n",
        "        'input_ids': ids, \n",
        "        'attention_mask': mask, \n",
        "        'labels': self.labels[idx]\n",
        "    }\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghOHuZ0iRNOW"
      },
      "source": [
        "import io\n",
        "\n",
        "def load_and_preprocess(ds_path):\n",
        "  print(f'Loading data from {ds_path}')\n",
        "  label_ids = []\n",
        "  encodings = []\n",
        "  with io.open(dataset_dir + ds_path, encoding=\"utf8\") as f:\n",
        "    reader = torchtext.utils.unicode_csv_reader(f)\n",
        "    next(reader)  # skip header\n",
        "    for line in reader:\n",
        "      label_ids.append(label2id[line[3]])\n",
        "      encodings.append(tokenizer(line[5], truncation=True, padding='max_length', max_length=S))\n",
        "  return VictorDataset(encodings, label_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjFfq9KNsLi8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd4975a-1b05-4d6f-afc4-f7ea0d982bfe"
      },
      "source": [
        "%%time\n",
        "\n",
        "ds_test = load_and_preprocess(test_ds_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data from test_small.csv\n",
            "CPU times: user 1min 5s, sys: 2.72 s, total: 1min 8s\n",
            "Wall time: 1min 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l7hwX9t-VWj"
      },
      "source": [
        "### Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCGmfh5F2JUW"
      },
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric('f1')\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  labels = eval_pred.label_ids\n",
        "  predictions = eval_pred.predictions.argmax(-1)\n",
        "  f1_macro = metric.compute(predictions=predictions, references=labels, average='macro')\n",
        "  f1_weighted = metric.compute(predictions=predictions, references=labels, average='weighted')\n",
        "  return {\n",
        "      'f1_macro': f1_macro['f1'], \n",
        "      'f1_weighted': f1_weighted['f1']\n",
        "  }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CbRN_dY-VWu"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
        "    output_dir='./logs'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, \n",
        "    args=training_args, \n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACcT_NE4i3-B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "fcc59590-5750-4b64-f573-d2a41de085a6"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "outputs = trainer.predict(test_dataset=ds_test)\n",
        "\n",
        "report = classification_report(\n",
        "    ds_test.labels, \n",
        "    np.argmax(outputs[0], axis=1), \n",
        "    digits=4, \n",
        "    target_names=['acordao_de_2_instancia', 'agravo_em_recurso_extraordinario', 'despacho_de_admissibilidade', 'outros', 'peticao_do_RE', 'sentenca'])\n",
        "\n",
        "print(report)\n",
        "\n",
        "rep_file = open(model_path + f'report-{test_ds_file}.txt', \"wt\")\n",
        "rep_file.write(f'{model_path}\\nS: {S}\\ndataset: {test_ds_file}\\n')\n",
        "rep_file.write(report)\n",
        "rep_file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='2986' max='2986' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2986/2986 26:49]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "                                  precision    recall  f1-score   support\n",
            "\n",
            "          acordao_de_2_instancia     0.7307    0.9341    0.8199       273\n",
            "agravo_em_recurso_extraordinario     0.4810    0.5915    0.5306      1841\n",
            "     despacho_de_admissibilidade     0.6828    0.6414    0.6615       198\n",
            "                          outros     0.9718    0.9607    0.9662     85408\n",
            "                   peticao_do_RE     0.7115    0.7504    0.7305      6331\n",
            "                        sentenca     0.7161    0.7831    0.7481      1475\n",
            "\n",
            "                        accuracy                         0.9362     95526\n",
            "                       macro avg     0.7156    0.7769    0.7428     95526\n",
            "                    weighted avg     0.9398    0.9362    0.9378     95526\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7LZ8yLmsBkl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}